<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Scalanetwork by nearbydelta</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Scalanetwork</h1>
        <p>A Neural network implementation with Scala</p>
        <p class="view"><a href="https://github.com/nearbydelta/ScalaNetwork">View the Project on GitHub <small>nearbydelta/ScalaNetwork</small></a></p>
        <ul>
          <li><a href="https://github.com/nearbydelta/ScalaNetwork/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/nearbydelta/ScalaNetwork/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/nearbydelta/ScalaNetwork">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="scalanetwork-014-snapshot" class="anchor" href="#scalanetwork-014-snapshot" aria-hidden="true"><span class="octicon octicon-link"></span></a>ScalaNetwork 0.1.4-SNAPSHOT</h1>

<p>A <em>Neural Network implementation</em> with Scala, <a href="https://github.com/scalanlp/breeze">Breeze</a> &amp; <a href="http://spark.apache.org">Spark</a></p>

<p>Spark Network follows <a href="http://choosealicense.com/licenses/gpl-2.0/">GPL v2 license</a>.</p>

<h1>
<a id="features" class="anchor" href="#features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features</h1>

<h2>
<a id="network" class="anchor" href="#network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network</h2>

<p>ScalaNetwork supports following layered neural network implementation:</p>

<ul>
<li>
<em>Fully-connected</em> Neural Network : f(Wx + b)</li>
<li>
<em>Fully-connected</em> Rank-3 Tensor Network : f(v<sub>1</sub><sup>T</sup>Q<sup>[1:k]</sup>v<sub>2</sub> + L<sup>[1:k]</sup>v + b)</li>
<li>
<em>Fully-connected</em> Auto Encoder</li>
<li>
<em>Fully-connected</em> Stacked Auto Encoder</li>
</ul>

<p>Also you can implement following Recursive Network via training tools.</p>

<ul>
<li>
<em>Recursive</em> Auto Encoder (RAE) <sup>[TEST-IN-PROGRESS]</sup>
</li>
<li>
<em>Recursive</em> General Neural Network (Including Recursive Neural Tensor Network, RNTN) <sup>[TEST-IN-PROGRESS]</sup>
</li>
</ul>

<h2>
<a id="training-methodology" class="anchor" href="#training-methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Methodology</h2>

<p>ScalaNetwork supports following training methodologies:</p>

<ul>
<li>Stochastic Gradient Descent w/ L1-, L2-regularization, Momentum.</li>
<li><a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">AdaGrad</a></li>
<li><a href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">AdaDelta</a></li>
</ul>

<p>ScalaNetwork supports following environments:</p>

<ul>
<li>Single-Threaded Training Environment.</li>
<li>Spark-based Distributed Environment, with modified version of Downpour SGD in <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">DistBelief</a>
</li>
</ul>

<h2>
<a id="activation-function" class="anchor" href="#activation-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation Function</h2>

<p>ScalaNetwork supports following activation functions:</p>

<ul>
<li>Sigmoid</li>
<li>HyperbolicTangent</li>
<li>Rectifier</li>
<li>Softplus</li>
</ul>

<h1>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h1>

<p>Here is some examples for basic usage. If you want to extend this package or use it more precisely, please refer <a href="http://nearbydelta.github.io/ScalaNetwork/api/#kr.ac.kaist.ir.deep.package">ScalaDoc</a></p>

<h2>
<a id="download" class="anchor" href="#download" aria-hidden="true"><span class="octicon octicon-link"></span></a>Download</h2>

<p>Currently ScalaNetwork supports Scala version 2.10 ~ 2.11.</p>

<ul>
<li>Stable Release is 0.1.3</li>
<li>Snapshot Release is 0.1.4-SNAPSHOT</li>
</ul>

<p>If you are using SBT, add a dependency as described below:</p>

<div class="highlight highlight-scala"><pre>libraryDependencies <span class="pl-k">+</span><span class="pl-k">=</span> <span class="pl-s1"><span class="pl-pds">"</span>kr.ac.kaist.ir<span class="pl-pds">"</span></span> <span class="pl-k">%%</span> <span class="pl-s1"><span class="pl-pds">"</span>scalanetwork<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s1"><span class="pl-pds">"</span>0.1.3<span class="pl-pds">"</span></span></pre></div>

<p>If you are using Maven, add a dependency as described below:</p>

<div class="highlight highlight-xml"><pre>&lt;<span class="pl-ent">dependency</span>&gt;
  &lt;<span class="pl-ent">groupId</span>&gt;kr.ac.kaist.ir&lt;/<span class="pl-ent">groupId</span>&gt;
  &lt;<span class="pl-ent">artifactId</span>&gt;scalanetwork_${your.scala.version}&lt;/<span class="pl-ent">artifactId</span>&gt;
  &lt;<span class="pl-ent">version</span>&gt;0.1.3&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre></div>

<h2>
<a id="simple-example" class="anchor" href="#simple-example" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple Example</h2>

<p><code>Network.apply(Activation, Int*)</code> generates fully-connected network:</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">// Define 2 -&gt; 4 -&gt; 1 Layered, Fully connected network.</span>
<span class="pl-k">val</span> <span class="pl-en">net</span> <span class="pl-k">=</span> <span class="pl-en">Network</span>(<span class="pl-en">Sigmoid</span>, <span class="pl-c1">2</span>, <span class="pl-c1">4</span>, <span class="pl-c1">1</span>)
<span class="pl-c">// Define Training Style. SingleThreadTrainStyle vs DistBeliefTrainStyle</span>
<span class="pl-k">val</span> <span class="pl-en">style</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SingleThreadTrainStyle</span>[<span class="pl-en">ScalarMatrix</span>](
  net <span class="pl-k">=</span> net,
  algorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">StochasticGradientDescent</span>(l2decay <span class="pl-k">=</span> <span class="pl-c1">0.0001</span>),
  param <span class="pl-k">=</span> <span class="pl-en">SimpleTrainingCriteria</span>(miniBatch <span class="pl-k">=</span> <span class="pl-c1">8</span>))
<span class="pl-c">// Define Input Operation. ScalarVector vs TreeRAE vs TreeRecursive</span>
<span class="pl-k">val</span> <span class="pl-en">operation</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ScalarVector</span>(
  corrupt <span class="pl-k">=</span> <span class="pl-en">GaussianCorruption</span>(variance <span class="pl-k">=</span> <span class="pl-c1">0.1</span>)
)
<span class="pl-c">// Define Trainer</span>
<span class="pl-k">val</span> <span class="pl-en">train</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">Trainer</span>(
  style <span class="pl-k">=</span> style,
  make <span class="pl-k">=</span> operation,
  stops <span class="pl-k">=</span> <span class="pl-en">StoppingCriteria</span>(maxIter <span class="pl-k">=</span> <span class="pl-c1">100000</span>))
<span class="pl-c">// Do Train</span>
train.train(set, valid)</pre></div>

<h2>
<a id="network-creation" class="anchor" href="#network-creation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network Creation</h2>

<p>To create network, you can choose one of the followings:</p>

<ul>
<li>Most simplest : Using sugar syntax, <code>Network.apply</code>
</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-c">// Network(Activation, SizeOfLayer1, SizeOfLayer2, SizeOfLayer3, ...)</span>
<span class="pl-en">Network</span>(<span class="pl-en">Sigmoid</span>, <span class="pl-c1">2</span>, <span class="pl-c1">4</span>, <span class="pl-c1">1</span>)
<span class="pl-en">Network</span>(<span class="pl-en">HyperbolicTangent</span>, <span class="pl-c1">4</span>, <span class="pl-c1">10</span>, <span class="pl-c1">7</span>)
<span class="pl-en">Network</span>(<span class="pl-en">Rectifier</span>, <span class="pl-c1">30</span>, <span class="pl-c1">10</span>, <span class="pl-c1">5</span>)
<span class="pl-en">Network</span>(<span class="pl-en">Softplus</span>, <span class="pl-c1">100</span>, <span class="pl-c1">50</span>, <span class="pl-c1">30</span>, <span class="pl-c1">10</span>, <span class="pl-c1">1</span>)</pre></div>

<ul>
<li>If you want different activation functions for each layer,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">layer1</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">BasicLayer</span>(<span class="pl-c1">10</span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">7</span>, <span class="pl-en">Sigmoid</span>)
<span class="pl-k">val</span> <span class="pl-en">layer2</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SplitTensorLayer</span>((<span class="pl-c1">3</span>, <span class="pl-c1">4</span>) <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">2</span>, <span class="pl-en">Rectifier</span>)
<span class="pl-k">new</span> <span class="pl-en">BasicNetwork</span>(<span class="pl-en">Seq</span>(layer1, layer2), <span class="pl-c1">0.95</span>)</pre></div>

<p>Second argument of Basic Network indicates presence probability, 
i.e. 1 - (neuron drop-out probability for drop-out training). Default is 1.</p>

<ul>
<li>If you want single-layer AutoEncoder,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">layer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ReconBasicLayer</span>(<span class="pl-c1">10</span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">7</span>, <span class="pl-en">Sigmoid</span>)
<span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(layer, <span class="pl-c1">0.95</span>)</pre></div>

<p>AutoEncoder only accepts <code>Reconstructable</code> type. Currently, <code>ReconBasicLayer</code> is only supported one. 
(Tensor layer version is planned)</p>

<ul>
<li>If you want to stack autoencoders,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">net1</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(...)
<span class="pl-k">val</span> <span class="pl-en">net2</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(...)
<span class="pl-k">new</span> <span class="pl-en">StackedAutoEncoder</span>(<span class="pl-en">Seq</span>(net1, net2))</pre></div>

<p>Note that StackedAutoEncoder does not get any presence probability.</p>

<h2>
<a id="training" class="anchor" href="#training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h2>

<h3>
<a id="algorithm--training-criteria" class="anchor" href="#algorithm--training-criteria" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm &amp; Training Criteria</h3>

<p>Before choose Training Style, you must specify algorithm and training criteria.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Algorithms */</span>

<span class="pl-k">new</span> <span class="pl-en">StochasticGradientDescent</span>(rate<span class="pl-k">=</span><span class="pl-c1">0.8</span>, l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>, momentum<span class="pl-k">=</span><span class="pl-c1">0.0001</span>)
<span class="pl-k">new</span> <span class="pl-en">AdaGrad</span>(rate<span class="pl-k">=</span><span class="pl-c1">0.6</span>, l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>)
<span class="pl-k">new</span> <span class="pl-en">AdaDelta</span>(l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>, decay<span class="pl-k">=</span><span class="pl-c1">0.95</span>, epsilon<span class="pl-k">=</span><span class="pl-c1">1e-6</span>)

<span class="pl-c">/* Training Criteria */</span>
<span class="pl-en">SimpleTrainingCriteria</span>(miniBatch<span class="pl-k">=</span><span class="pl-c1">100</span>, validationSize<span class="pl-k">=</span><span class="pl-c1">20</span>)
<span class="pl-en">DistBeliefCriteria</span>(miniBatch<span class="pl-k">=</span><span class="pl-c1">100</span>, validationSize<span class="pl-k">=</span><span class="pl-c1">20</span>, updateStep<span class="pl-k">=</span><span class="pl-c1">2</span>, fetchStep<span class="pl-k">=</span><span class="pl-c1">10</span>, numCores<span class="pl-k">=</span><span class="pl-c1">1</span>)</pre></div>

<p>Validation size sets the number of elements used for validation phrase.</p>

<h3>
<a id="training-style" class="anchor" href="#training-style" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Style</h3>

<p>You can choose the training style of the network.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Styles */</span>
<span class="pl-k">new</span> <span class="pl-en">SingleThreadTrainStyle</span>(net, algorithm, param)
<span class="pl-k">new</span> <span class="pl-en">DistBeliefTrainStyle</span>(net, sparkContext, algorithm, param<span class="pl-k">:</span><span class="pl-en">DistBeliefCriteria</span>)</pre></div>

<h3>
<a id="input-options" class="anchor" href="#input-options" aria-hidden="true"><span class="octicon octicon-link"></span></a>Input Options</h3>

<p>Also you can specify input operations or options.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Corruptions */</span>
<span class="pl-en">NoCorruption</span>
<span class="pl-en">DroppingCorruption</span>(presence<span class="pl-k">=</span><span class="pl-c1">0.95</span>)
<span class="pl-en">GaussianCorruption</span>(mean<span class="pl-k">=</span><span class="pl-c1">0</span>, variance<span class="pl-k">=</span><span class="pl-c1">0.1</span>)

<span class="pl-c">/* Objective Functions */</span>
<span class="pl-en">SquaredErr</span>
<span class="pl-en">CrossEntropyErr</span> <span class="pl-c">// Which is Logistic Err</span>

<span class="pl-c">/* Vector Input */</span>
<span class="pl-k">new</span> <span class="pl-en">ScalarVector</span>(corrupt, objective)

<span class="pl-c">/* Tree Input */</span>
<span class="pl-c">// Train network as RAE style. </span>
<span class="pl-c">// Every internal node regarded as reconstruction its direct children (not all leaves).</span>
<span class="pl-k">new</span> <span class="pl-en">TreeRAE</span>(corrupt, objective)
<span class="pl-c">// Train network as Recursive Network style(for RNTN). </span>
<span class="pl-c">// Forward propagation is done on whole tree at once, and then propagate back.</span>
<span class="pl-k">new</span> <span class="pl-en">TreeRecursive</span>(corrupt, objective)
<span class="pl-c">// Experimental: Train network as URAE style. </span>
<span class="pl-c">// With same structure, network should reconstruct all leaves from root.</span>
<span class="pl-k">new</span> <span class="pl-en">TreeURAE</span>(corrupt, objective)</pre></div>

<h3>
<a id="training-1" class="anchor" href="#training-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h3>

<p>Training is done by <code>Trainer</code> class.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Stopping Criteria */</span>
<span class="pl-en">StoppingCriteria</span>(maxIter <span class="pl-k">=</span> <span class="pl-c1">100000</span>, patience<span class="pl-k">=</span> <span class="pl-c1">5000</span>, patienceStep<span class="pl-k">=</span><span class="pl-c1">2</span>, 
  improveThreshold<span class="pl-k">=</span><span class="pl-c1">0.95</span>, lossThreshold<span class="pl-k">=</span><span class="pl-c1">1e-4</span>, validationFreq<span class="pl-k">=</span><span class="pl-c1">100</span>)

<span class="pl-c">/* Trainer */</span>
<span class="pl-k">new</span> <span class="pl-en">Trainer</span>(style <span class="pl-k">=</span> style, make <span class="pl-k">=</span> operation, stops <span class="pl-k">=</span> <span class="pl-en">StoppingCriteria</span>())</pre></div>

<ul>
<li>
<strong>Patience</strong> and <strong>its step</strong> indicates wating time from the improvement. If network output improved on 100-th iteration, 
the trainer waits until <code>Max(patience, 100 * patienceStep)</code>.</li>
<li>
<strong>Improve Threshold</strong> indicates bottom line for improvement. 
To be regarded as improved, loss should be less than (best loss) * improveThreshold</li>
<li>
<strong>Loss threshold</strong> indicates maximum loss can be accepted.</li>
<li>
<strong>Validation Frequency</strong> sets the number of iterations between validations.</li>
</ul>

<p>Training is done by <code>train</code> method.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">// If training and validation set are the same</span>
trainer.train(<span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)])
trainer.train(<span class="pl-st">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)]) <span class="pl-c">// With generator.</span>

<span class="pl-c">// If they are different</span>
trainer.train(<span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)], <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)])
trainer.train(<span class="pl-st">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)], <span class="pl-st">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)])

<span class="pl-c">// If you are using RDD</span>
trainer.train(<span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)])
trainer.train(<span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)], <span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">ScalarMatrix</span>)])</pre></div>

<p>If you are using RDD, ScalaNetwork automatically caches your input sequence.</p>

<h1>
<a id="blueprint" class="anchor" href="#blueprint" aria-hidden="true"><span class="octicon octicon-link"></span></a>Blueprint</h1>

<p>ScalaNetwork will support these implementations:</p>

<ul>
<li>Unfolded Recursive Auto Encoder (URAE)</li>
</ul>

<p>Also ScalaNetwork will support these features:</p>

<ul>
<li>Input-dependent Weight</li>
</ul>

<h2>
<a id="current-status" class="anchor" href="#current-status" aria-hidden="true"><span class="octicon octicon-link"></span></a>Current Status</h2>

<p>Next version(v0.2) will support URAE</p>
      </section>
    </div>
    <footer>
      <p>Project maintained by <a href="https://github.com/nearbydelta">nearbydelta</a></p>
      <p>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></p>
    </footer>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>