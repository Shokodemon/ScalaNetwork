<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Scalanetwork by nearbydelta</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Scalanetwork</h1>
      <h2 class="project-tagline">A Neural network implementation with Scala</h2>
      <a href="https://github.com/nearbydelta/ScalaNetwork" class="btn">View on GitHub</a>
      <a href="https://github.com/nearbydelta/ScalaNetwork/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/nearbydelta/ScalaNetwork/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="scalanetwork-102" class="anchor" href="#scalanetwork-102" aria-hidden="true"><span class="octicon octicon-link"></span></a>ScalaNetwork 1.0.2</h1>

<p>A <em>Neural Network implementation</em> with Scala, <a href="https://github.com/scalanlp/breeze">Breeze</a> &amp; <a href="http://spark.apache.org">Spark</a></p>

<p>Spark Network follows <a href="http://choosealicense.com/licenses/gpl-2.0/">GPL v2 license</a>.</p>

<h1>
<a id="features" class="anchor" href="#features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Features</h1>

<h2>
<a id="network" class="anchor" href="#network" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network</h2>

<p>ScalaNetwork supports following layered neural network implementation:</p>

<ul>
<li>
<em>Fully-connected</em> Neural Network : f(Wx + b)</li>
<li>
<em>Fully-connected</em> Rank-3 Tensor Network : f(v<sub>1</sub><sup>T</sup>Q<sup>[1:k]</sup>v<sub>2</sub> + L<sup>[1:k]</sup>v + b)</li>
<li>
<em>Fully-connected</em> Auto Encoder</li>
<li>
<em>Fully-connected</em> Stacked Auto Encoder</li>
</ul>

<p>Also you can implement following Recursive Network via training tools.</p>

<ul>
<li>Traditional <em>Recursive</em> Auto Encoder (RAE)</li>
<li>Standard <em>Recursive</em> Auto Encoder (RAE)</li>
<li>Unfolding <em>Recursive</em> Auto Encoder (RAE) <sup>[EXPERIMENTAL]</sup>
</li>
</ul>

<h2>
<a id="training-methodology" class="anchor" href="#training-methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Methodology</h2>

<p>ScalaNetwork supports following training methodologies:</p>

<ul>
<li>Stochastic Gradient Descent w/ L1-, L2-regularization, Momentum.</li>
<li><a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">AdaGrad</a></li>
<li><a href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">AdaDelta</a></li>
</ul>

<p>ScalaNetwork supports following environments:</p>

<ul>
<li>Single-Threaded Training Environment.</li>
<li>Spark-based Distributed Environment, with modified version of Downpour SGD in <a href="http://research.google.com/archive/large_deep_networks_nips2012.html">DistBelief</a>
</li>
</ul>

<p>Also you can add negative examples with <code>Trainer.setNegativeSampler()</code>.</p>

<h2>
<a id="activation-function" class="anchor" href="#activation-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation Function</h2>

<p>ScalaNetwork supports following activation functions:</p>

<ul>
<li>Linear</li>
<li>Sigmoid</li>
<li>HyperbolicTangent</li>
<li>Rectifier</li>
<li>Softplus</li>
<li>HardSigmoid</li>
<li>HardTanh</li>
<li>Softmax</li>
</ul>

<p>And also you can make new activation function using several operations.</p>

<h1>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h1>

<p>Here is some examples for basic usage. If you want to extend this package or use it more precisely, please refer <a href="http://nearbydelta.github.io/ScalaNetwork/api/#kr.ac.kaist.ir.deep.package">ScalaDoc</a></p>

<h2>
<a id="download" class="anchor" href="#download" aria-hidden="true"><span class="octicon octicon-link"></span></a>Download</h2>

<p>Currently ScalaNetwork supports Scala version 2.10 ~ 2.11.</p>

<ul>
<li>Stable Release is 1.0.2</li>
</ul>

<p>If you are using SBT, add a dependency as described below:</p>

<div class="highlight highlight-scala"><pre>libraryDependencies <span class="pl-k">+</span><span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>kr.ac.kaist.ir<span class="pl-pds">"</span></span> <span class="pl-k">%%</span> <span class="pl-s"><span class="pl-pds">"</span>scalanetwork<span class="pl-pds">"</span></span> <span class="pl-k">%</span> <span class="pl-s"><span class="pl-pds">"</span>1.0.2<span class="pl-pds">"</span></span></pre></div>

<p>If you are using Maven, add a dependency as described below:</p>

<div class="highlight highlight-xml"><pre>&lt;<span class="pl-ent">dependency</span>&gt;
  &lt;<span class="pl-ent">groupId</span>&gt;kr.ac.kaist.ir&lt;/<span class="pl-ent">groupId</span>&gt;
  &lt;<span class="pl-ent">artifactId</span>&gt;scalanetwork_${your.scala.version}&lt;/<span class="pl-ent">artifactId</span>&gt;
  &lt;<span class="pl-ent">version</span>&gt;1.0.1&lt;/<span class="pl-ent">version</span>&gt;
&lt;/<span class="pl-ent">dependency</span>&gt;</pre></div>

<h2>
<a id="simple-example" class="anchor" href="#simple-example" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simple Example</h2>

<p><code>Network.apply(Activation, Int*)</code> generates fully-connected network:</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">// Define 2 -&gt; 4 -&gt; 1 Layered, Fully connected network.</span>
<span class="pl-k">val</span> <span class="pl-en">net</span> <span class="pl-k">=</span> <span class="pl-en">Network</span>(<span class="pl-en">Sigmoid</span>, <span class="pl-c1">2</span>, <span class="pl-c1">4</span>, <span class="pl-c1">1</span>)
<span class="pl-c">// Define Manipulation Type. VectorType, AEType, RAEType, StandardRAEType, URAEType, and StringToVectorType.</span>
<span class="pl-k">val</span> <span class="pl-en">operation</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">VectorType</span>(
   corrupt <span class="pl-k">=</span> <span class="pl-en">GaussianCorruption</span>(variance <span class="pl-k">=</span> <span class="pl-c1">0.1</span>)
)
<span class="pl-c">// Define Training Style. SingleThreadTrainStyle, MultiThreadTrainStyle, &amp; DistBeliefTrainStyle</span>
<span class="pl-k">val</span> <span class="pl-en">style</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SingleThreadTrainStyle</span>(
  net <span class="pl-k">=</span> net,
  algorithm <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">StochasticGradientDescent</span>(l2decay <span class="pl-k">=</span> <span class="pl-c1">0.0001f</span>),
  make <span class="pl-k">=</span> operation,
  param <span class="pl-k">=</span> <span class="pl-en">SimpleTrainingCriteria</span>(miniBatchFraction <span class="pl-k">=</span> <span class="pl-c1">0.01f</span>))
<span class="pl-c">// Define Trainer</span>
<span class="pl-k">val</span> <span class="pl-en">train</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">Trainer</span>(
  style <span class="pl-k">=</span> style,
  stops <span class="pl-k">=</span> <span class="pl-en">StoppingCriteria</span>(maxIter <span class="pl-k">=</span> <span class="pl-c1">100000</span>))
<span class="pl-c">// Do Train</span>
train.train(set, valid)</pre></div>

<h2>
<a id="network-creation" class="anchor" href="#network-creation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Network Creation</h2>

<p>To create network, you can choose one of the followings:</p>

<ul>
<li>Most simplest : Using sugar syntax, <code>Network.apply</code>
</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-c">// Network(Activation, SizeOfLayer1, SizeOfLayer2, SizeOfLayer3, ...)</span>
<span class="pl-en">Network</span>(<span class="pl-en">Sigmoid</span>, <span class="pl-c1">2</span>, <span class="pl-c1">4</span>, <span class="pl-c1">1</span>)
<span class="pl-en">Network</span>(<span class="pl-en">HyperbolicTangent</span>, <span class="pl-c1">4</span>, <span class="pl-c1">10</span>, <span class="pl-c1">7</span>)
<span class="pl-en">Network</span>(<span class="pl-en">Rectifier</span>, <span class="pl-c1">30</span>, <span class="pl-c1">10</span>, <span class="pl-c1">5</span>)
<span class="pl-en">Network</span>(<span class="pl-en">Softplus</span>, <span class="pl-c1">100</span>, <span class="pl-c1">50</span>, <span class="pl-c1">30</span>, <span class="pl-c1">10</span>, <span class="pl-c1">1</span>)</pre></div>

<ul>
<li>If you want different activation functions for each layer,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">layer1</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">BasicLayer</span>(<span class="pl-c1">10</span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">7</span>, <span class="pl-en">Sigmoid</span>)
<span class="pl-k">val</span> <span class="pl-en">layer2</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">SplitTensorLayer</span>((<span class="pl-c1">3</span>, <span class="pl-c1">4</span>) <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">2</span>, <span class="pl-en">Rectifier</span>)
<span class="pl-k">new</span> <span class="pl-en">BasicNetwork</span>(<span class="pl-en">Seq</span>(layer1, layer2), <span class="pl-c1">0.95</span>)</pre></div>

<p>Second argument of Basic Network indicates presence probability, 
i.e. 1 - (neuron drop-out probability for drop-out training). Default is 1.</p>

<ul>
<li>If you want single-layer AutoEncoder,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">layer</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">ReconBasicLayer</span>(<span class="pl-c1">10</span> <span class="pl-k">-</span><span class="pl-k">&gt;</span> <span class="pl-c1">7</span>, <span class="pl-en">Sigmoid</span>)
<span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(layer, <span class="pl-c1">0.95</span>)</pre></div>

<p>AutoEncoder only accepts <code>Reconstructable</code> type. Currently, <code>ReconBasicLayer</code> is only supported one. 
(Tensor layer version is planned)</p>

<ul>
<li>If you want to stack autoencoders,</li>
</ul>

<div class="highlight highlight-scala"><pre><span class="pl-k">val</span> <span class="pl-en">net1</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(...)
<span class="pl-k">val</span> <span class="pl-en">net2</span> <span class="pl-k">=</span> <span class="pl-k">new</span> <span class="pl-en">AutoEncoder</span>(...)
<span class="pl-k">new</span> <span class="pl-en">StackedAutoEncoder</span>(<span class="pl-en">Seq</span>(net1, net2))</pre></div>

<p>Note that StackedAutoEncoder does not get any presence probability.</p>

<h2>
<a id="training" class="anchor" href="#training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h2>

<h3>
<a id="algorithm--training-criteria" class="anchor" href="#algorithm--training-criteria" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm &amp; Training Criteria</h3>

<p>Before choose Training Style, you must specify algorithm and training criteria.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Algorithms */</span>
<span class="pl-k">new</span> <span class="pl-en">StochasticGradientDescent</span>(rate<span class="pl-k">=</span><span class="pl-c1">0.8</span>, l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>, momentum<span class="pl-k">=</span><span class="pl-c1">0.0001</span>)
<span class="pl-k">new</span> <span class="pl-en">AdaGrad</span>(rate<span class="pl-k">=</span><span class="pl-c1">0.6</span>, l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>)
<span class="pl-k">new</span> <span class="pl-en">AdaDelta</span>(l1decay<span class="pl-k">=</span><span class="pl-c1">0.0</span>, l2decay<span class="pl-k">=</span><span class="pl-c1">0.0001</span>, decay<span class="pl-k">=</span><span class="pl-c1">0.95</span>, epsilon<span class="pl-k">=</span><span class="pl-c1">1e-6</span>)</pre></div>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Training Criteria */</span>
<span class="pl-k">import</span> <span class="pl-v">scala.concurrent.duration.</span><span class="pl-v">_</span>
<span class="pl-en">SimpleTrainingCriteria</span>(miniBatchFraction<span class="pl-k">=</span><span class="pl-c1">0.01f</span>, validationSize<span class="pl-k">=</span><span class="pl-c1">20</span>)
<span class="pl-en">DistBeliefCriteria</span>(miniBatchFraction<span class="pl-k">=</span><span class="pl-c1">0.01f</span>, validationSize<span class="pl-k">=</span><span class="pl-c1">20</span>, submitInterval<span class="pl-k">=</span><span class="pl-c1">1.</span>seconds,
  updateStep<span class="pl-k">=</span><span class="pl-c1">2</span>, fetchStep<span class="pl-k">=</span><span class="pl-c1">10</span>, numCores<span class="pl-k">=</span><span class="pl-c1">1</span>, repartitionOnStart <span class="pl-k">=</span> <span class="pl-c1">true</span>, storageLevel <span class="pl-k">=</span> <span class="pl-en">StorageLevel</span>.<span class="pl-en">MEMORY_ONLY</span>)</pre></div>

<p>Validation size sets the number of elements used for validation phrase.</p>

<h3>
<a id="input-options" class="anchor" href="#input-options" aria-hidden="true"><span class="octicon octicon-link"></span></a>Input Options</h3>

<p>Also you can specify input operations or options.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Corruptions */</span>
<span class="pl-en">NoCorruption</span>
<span class="pl-en">DroppingCorruption</span>(presence<span class="pl-k">=</span><span class="pl-c1">0.95</span>)
<span class="pl-en">GaussianCorruption</span>(mean<span class="pl-k">=</span><span class="pl-c1">0</span>, variance<span class="pl-k">=</span><span class="pl-c1">0.1</span>)</pre></div>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Objective Functions */</span>
<span class="pl-en">SquaredErr</span>
<span class="pl-en">CrossEntropyErr</span> <span class="pl-c">// Which is Logistic Err</span></pre></div>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Manipulation Type : Vector input, Vector output */</span>
<span class="pl-c">// General Neural Network type</span>
<span class="pl-k">new</span> <span class="pl-en">VectorType</span>(corrupt, objective)
<span class="pl-c">// General AutoEncoder type</span>
<span class="pl-k">new</span> <span class="pl-en">AEType</span>(corrupt, objective)

<span class="pl-c">/* Manipulation Type : Tree input, Null output (AutoEncoder) */</span>
<span class="pl-c">// Train network as RAE style. </span>
<span class="pl-c">// Every internal node regarded as reconstruction its direct children (not all leaves).</span>
<span class="pl-k">new</span> <span class="pl-en">RAEType</span>(corrupt, objective)
<span class="pl-k">new</span> <span class="pl-en">StandardRAEType</span>(corrupt, objective)
<span class="pl-c">// Experimental: Train network as URAE style. </span>
<span class="pl-c">// With same structure, network should reconstruct all leaves from root.</span>
<span class="pl-k">new</span> <span class="pl-en">URAEType</span>(corrupt, objective)

<span class="pl-c">/* Manipulation Type : String input, Vector output */</span>
<span class="pl-k">new</span> <span class="pl-en">StringToVectorType</span>(model, objective)</pre></div>

<h3>
<a id="training-style" class="anchor" href="#training-style" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Style</h3>

<p>You can choose the training style of the network.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Styles */</span>
<span class="pl-k">new</span> <span class="pl-en">SingleThreadTrainStyle</span>(net, algorithm, make<span class="pl-k">:</span><span class="pl-en">ManipulationType</span>, param)
<span class="pl-k">new</span> <span class="pl-en">MultiThreadTrainStyle</span>(net, sparkContext, algorithm, make<span class="pl-k">:</span><span class="pl-en">ManipulationType</span>, param<span class="pl-k">:</span><span class="pl-en">DistBeliefCriteria</span>)
<span class="pl-k">new</span> <span class="pl-en">DistBeliefTrainStyle</span>(net, sparkContext, algorithm, make<span class="pl-k">:</span><span class="pl-en">ManipulationType</span>, param<span class="pl-k">:</span><span class="pl-en">DistBeliefCriteria</span>)</pre></div>

<h3>
<a id="training-1" class="anchor" href="#training-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training</h3>

<p>Training is done by <code>Trainer</code> class.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">/* Stopping Criteria */</span>
<span class="pl-en">StoppingCriteria</span>(maxIter <span class="pl-k">=</span> <span class="pl-c1">100000</span>, waitAfterUpdate<span class="pl-k">=</span><span class="pl-c1">2</span>,
  improveThreshold<span class="pl-k">=</span><span class="pl-c1">0.95</span>, lossThreshold<span class="pl-k">=</span><span class="pl-c1">1e-4</span>, validationFreq<span class="pl-k">=</span><span class="pl-c1">1.0f</span>)

<span class="pl-c">/* Trainer */</span>
<span class="pl-k">new</span> <span class="pl-en">Trainer</span>(style <span class="pl-k">=</span> style, stops <span class="pl-k">=</span> <span class="pl-en">StoppingCriteria</span>(), name <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Trainer<span class="pl-pds">"</span></span>)</pre></div>

<ul>
<li>
<strong>waitAfterUpdate</strong> indicates wating time from the improvement. If network output improved on 100-th iteration,
the trainer waits until <code>Max(validationEpoch, 100 * patienceStep)</code>.</li>
<li>
<strong>Improve Threshold</strong> indicates bottom line for improvement. 
To be regarded as improved, loss should be less than (best loss) * improveThreshold</li>
<li>
<strong>Loss threshold</strong> indicates maximum loss can be accepted.</li>
<li>
<strong>Validation Frequency</strong> sets the number of iterations between validations. (1 iteration does train all training examples)</li>
</ul>

<p>Training is done by <code>train</code> method.</p>

<div class="highlight highlight-scala"><pre><span class="pl-c">// If training and validation set are the same</span>
trainer.train(<span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)])
trainer.train(<span class="pl-k">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)]) <span class="pl-c">// With generator.</span>

<span class="pl-c">// If they are different</span>
trainer.train(<span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)], <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)])
trainer.train(<span class="pl-k">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)], <span class="pl-k">Int</span> <span class="pl-k">=&gt;</span> <span class="pl-en">Seq</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)])

<span class="pl-c">// If you are using RDD</span>
trainer.train(<span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)])
trainer.train(<span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)], <span class="pl-en">RDD</span>[(<span class="pl-en">IN</span>, <span class="pl-en">OUT</span>)])</pre></div>

<p>If you are using RDD, ScalaNetwork automatically caches your input sequence.</p>

<p>Also you can add negative examples, using <code>trainer.setNegativeTrainingReference()</code></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/nearbydelta/ScalaNetwork">Scalanetwork</a> is maintained by <a href="https://github.com/nearbydelta">nearbydelta</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

