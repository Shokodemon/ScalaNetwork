{"name":"Scalanetwork","tagline":"A Neural network implementation with Scala","body":"ScalaNetwork 0.1.4-SNAPSHOT\r\n===========================\r\n\r\nA *Neural Network implementation* with Scala, [Breeze](https://github.com/scalanlp/breeze) & [Spark](http://spark.apache.org)\r\n\r\nSpark Network follows [GPL v2 license](http://choosealicense.com/licenses/gpl-2.0/).\r\n\r\n# Features\r\n\r\n## Network\r\n\r\nScalaNetwork supports following layered neural network implementation:\r\n\r\n* *Fully-connected* Neural Network : f(Wx + b)\r\n* *Fully-connected* Rank-3 Tensor Network : f(v<sub>1</sub><sup>T</sup>Q<sup>[1:k]</sup>v<sub>2</sub> + L<sup>[1:k]</sup>v + b)\r\n* *Fully-connected* Auto Encoder\r\n* *Fully-connected* Stacked Auto Encoder\r\n\r\nAlso you can implement following Recursive Network via training tools.\r\n\r\n* *Recursive* Auto Encoder (RAE) <sup>[TEST-IN-PROGRESS]</sup>\r\n* *Recursive* General Neural Network (Including Recursive Neural Tensor Network, RNTN) <sup>[TEST-IN-PROGRESS]</sup>\r\n\r\n## Training Methodology\r\n\r\nScalaNetwork supports following training methodologies:\r\n\r\n* Stochastic Gradient Descent w/ L1-, L2-regularization, Momentum.\r\n* [AdaGrad](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)\r\n* [AdaDelta](http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf)\r\n\r\nScalaNetwork supports following environments:\r\n\r\n* Single-Threaded Training Environment.\r\n* Spark-based Distributed Environment, with modified version of Downpour SGD in [DistBelief](http://research.google.com/archive/large_deep_networks_nips2012.html)\r\n\r\n## Activation Function\r\n\r\nScalaNetwork supports following activation functions:\r\n\r\n* Sigmoid\r\n* HyperbolicTangent\r\n* Rectifier\r\n* Softplus\r\n\r\n# Usage\r\n\r\nHere is some examples for basic usage. If you want to extend this package or use it more precisely, please refer [ScalaDoc](http://nearbydelta.github.io/ScalaNetwork/api/#kr.ac.kaist.ir.deep.package)\r\n\r\n## Download\r\n\r\nCurrently ScalaNetwork supports Scala version 2.10 ~ 2.11.\r\n\r\n* Stable Release is 0.1.3\r\n* Snapshot Release is 0.1.4-SNAPSHOT\r\n \r\nIf you are using SBT, add a dependency as described below:\r\n\r\n```scala\r\nlibraryDependencies += \"kr.ac.kaist.ir\" %% \"scalanetwork\" % \"0.1.3\"\r\n```\r\n\r\nIf you are using Maven, add a dependency as described below:\r\n```xml\r\n<dependency>\r\n  <groupId>kr.ac.kaist.ir</groupId>\r\n  <artifactId>scalanetwork_${your.scala.version}</artifactId>\r\n  <version>0.1.3</version>\r\n</dependency>\r\n```\r\n\r\n## Simple Example\r\n`Network.apply(Activation, Int*)` generates fully-connected network:\r\n\r\n```scala\r\n// Define 2 -> 4 -> 1 Layered, Fully connected network.\r\nval net = Network(Sigmoid, 2, 4, 1)\r\n// Define Training Style. SingleThreadTrainStyle vs DistBeliefTrainStyle\r\nval style = new SingleThreadTrainStyle[ScalarMatrix](\r\n  net = net,\r\n  algorithm = new StochasticGradientDescent(l2decay = 0.0001),\r\n  param = SimpleTrainingCriteria(miniBatch = 8))\r\n// Define Input Operation. ScalarVector vs TreeRAE vs TreeRecursive\r\nval operation = new ScalarVector(\r\n  corrupt = GaussianCorruption(variance = 0.1)\r\n)\r\n// Define Trainer\r\nval train = new Trainer(\r\n  style = style,\r\n  make = operation,\r\n  stops = StoppingCriteria(maxIter = 100000))\r\n// Do Train\r\ntrain.train(set, valid)\r\n```\r\n\r\n## Network Creation\r\n\r\nTo create network, you can choose one of the followings:\r\n\r\n* Most simplest : Using sugar syntax, `Network.apply`\r\n\r\n```scala\r\n// Network(Activation, SizeOfLayer1, SizeOfLayer2, SizeOfLayer3, ...)\r\nNetwork(Sigmoid, 2, 4, 1)\r\nNetwork(HyperbolicTangent, 4, 10, 7)\r\nNetwork(Rectifier, 30, 10, 5)\r\nNetwork(Softplus, 100, 50, 30, 10, 1)\r\n```\r\n\r\n* If you want different activation functions for each layer,\r\n\r\n```scala\r\nval layer1 = new BasicLayer(10 -> 7, Sigmoid)\r\nval layer2 = new SplitTensorLayer((3, 4) -> 2, Rectifier)\r\nnew BasicNetwork(Seq(layer1, layer2), 0.95)\r\n```\r\n\r\nSecond argument of Basic Network indicates presence probability, \r\ni.e. 1 - (neuron drop-out probability for drop-out training). Default is 1.\r\n\r\n* If you want single-layer AutoEncoder,\r\n\r\n```scala\r\nval layer = new ReconBasicLayer(10 -> 7, Sigmoid)\r\nnew AutoEncoder(layer, 0.95)\r\n```\r\n\r\nAutoEncoder only accepts `Reconstructable` type. Currently, `ReconBasicLayer` is only supported one. \r\n(Tensor layer version is planned)\r\n\r\n* If you want to stack autoencoders,\r\n\r\n```scala\r\nval net1 = new AutoEncoder(...)\r\nval net2 = new AutoEncoder(...)\r\nnew StackedAutoEncoder(Seq(net1, net2))\r\n```\r\n\r\nNote that StackedAutoEncoder does not get any presence probability.\r\n\r\n## Training\r\n\r\n### Algorithm & Training Criteria\r\nBefore choose Training Style, you must specify algorithm and training criteria.\r\n\r\n```scala\r\n/* Algorithms */\r\n\r\nnew StochasticGradientDescent(rate=0.8, l1decay=0.0, l2decay=0.0001, momentum=0.0001)\r\nnew AdaGrad(rate=0.6, l1decay=0.0, l2decay=0.0001)\r\nnew AdaDelta(l1decay=0.0, l2decay=0.0001, decay=0.95, epsilon=1e-6)\r\n\r\n/* Training Criteria */\r\nSimpleTrainingCriteria(miniBatch=100, validationSize=20)\r\nDistBeliefCriteria(miniBatch=100, validationSize=20, updateStep=2, fetchStep=10, numCores=1)\r\n```\r\n\r\nValidation size sets the number of elements used for validation phrase.\r\n\r\n### Training Style\r\nYou can choose the training style of the network.\r\n\r\n```scala\r\n/* Styles */\r\nnew SingleThreadTrainStyle(net, algorithm, param)\r\nnew DistBeliefTrainStyle(net, sparkContext, algorithm, param:DistBeliefCriteria)\r\n```\r\n\r\n### Input Options\r\nAlso you can specify input operations or options.\r\n\r\n```scala\r\n/* Corruptions */\r\nNoCorruption\r\nDroppingCorruption(presence=0.95)\r\nGaussianCorruption(mean=0, variance=0.1)\r\n\r\n/* Objective Functions */\r\nSquaredErr\r\nCrossEntropyErr // Which is Logistic Err\r\n\r\n/* Vector Input */\r\nnew ScalarVector(corrupt, objective)\r\n\r\n/* Tree Input */\r\n// Train network as RAE style. \r\n// Every internal node regarded as reconstruction its direct children (not all leaves).\r\nnew TreeRAE(corrupt, objective)\r\n// Train network as Recursive Network style(for RNTN). \r\n// Forward propagation is done on whole tree at once, and then propagate back.\r\nnew TreeRecursive(corrupt, objective)\r\n// Experimental: Train network as URAE style. \r\n// With same structure, network should reconstruct all leaves from root.\r\nnew TreeURAE(corrupt, objective)\r\n```\r\n\r\n### Training\r\nTraining is done by `Trainer` class.\r\n\r\n```scala\r\n/* Stopping Criteria */\r\nStoppingCriteria(maxIter = 100000, patience= 5000, patienceStep=2, \r\n  improveThreshold=0.95, lossThreshold=1e-4, validationFreq=100)\r\n\r\n/* Trainer */\r\nnew Trainer(style = style, make = operation, stops = StoppingCriteria())\r\n```\r\n\r\n* **Patience** and **its step** indicates wating time from the improvement. If network output improved on 100-th iteration, \r\n  the trainer waits until `Max(patience, 100 * patienceStep)`.\r\n* **Improve Threshold** indicates bottom line for improvement. \r\n  To be regarded as improved, loss should be less than (best loss) * improveThreshold\r\n* **Loss threshold** indicates maximum loss can be accepted.\r\n* **Validation Frequency** sets the number of iterations between validations.\r\n\r\nTraining is done by `train` method.\r\n\r\n```scala\r\n// If training and validation set are the same\r\ntrainer.train(Seq[(IN, ScalarMatrix)])\r\ntrainer.train(Int => Seq[(IN, ScalarMatrix)]) // With generator.\r\n\r\n// If they are different\r\ntrainer.train(Seq[(IN, ScalarMatrix)], Seq[(IN, ScalarMatrix)])\r\ntrainer.train(Int => Seq[(IN, ScalarMatrix)], Int => Seq[(IN, ScalarMatrix)])\r\n\r\n// If you are using RDD\r\ntrainer.train(RDD[(IN, ScalarMatrix)])\r\ntrainer.train(RDD[(IN, ScalarMatrix)], RDD[(IN, ScalarMatrix)])\r\n```\r\n\r\nIf you are using RDD, ScalaNetwork automatically caches your input sequence.\r\n\r\n# Blueprint\r\n\r\nScalaNetwork will support these implementations:\r\n\r\n* Unfolded Recursive Auto Encoder (URAE)\r\n\r\nAlso ScalaNetwork will support these features:\r\n\r\n* Input-dependent Weight\r\n\r\n## Current Status\r\n\r\nNext version(v0.2) will support URAE\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}